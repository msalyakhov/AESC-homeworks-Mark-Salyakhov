{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dvm6MmIGddQw"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgcJ2LJ2ddQz"
   },
   "source": [
    "## 1. Q-learning in the wild (3 pts)\n",
    "\n",
    "Here we use the qlearning agent on taxi env from openai gym.\n",
    "You will need to insert a few agent functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6uJVRBJddQ1"
   },
   "outputs": [],
   "source": [
    "import random,math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent():\n",
    "  \"\"\"\n",
    "    Q-Learning Agent\n",
    "\n",
    "    Instance variables you have access to\n",
    "      - self.epsilon (exploration prob)\n",
    "      - self.alpha (learning rate)\n",
    "      - self.discount (discount rate aka gamma)\n",
    "\n",
    "    Functions you should use\n",
    "      - self.getLegalActions(state)\n",
    "        which returns legal actions for a state\n",
    "      - self.getQValue(state,action)\n",
    "        which returns Q(state,action)\n",
    "      - self.setQValue(state,action,value)\n",
    "        which sets Q(state,action) := value\n",
    "\n",
    "    !!!Important!!!\n",
    "    NOTE: please avoid using self._qValues directly to make code cleaner\n",
    "  \"\"\"\n",
    "  def __init__(self,alpha,epsilon,discount,getLegalActions):\n",
    "    \"We initialize agent and Q-values here.\"\n",
    "    self.getLegalActions= getLegalActions\n",
    "    self._qValues = defaultdict(lambda:defaultdict(lambda:0))\n",
    "    self.alpha = alpha\n",
    "    self.epsilon = epsilon\n",
    "    self.discount = discount\n",
    "\n",
    "  def getQValue(self, state, action):\n",
    "    #print(state)\n",
    "    #print(action)\n",
    "    if not (state in self._qValues) or not (action in self._qValues[state]):\n",
    "        return 0.0\n",
    "    return self._qValues[state][action]\n",
    "\n",
    "  def setQValue(self,state,action,value):\n",
    "    \"\"\"\n",
    "      Sets the Qvalue for [state,action] to the given value\n",
    "    \"\"\"\n",
    "    self._qValues[state][action] = value\n",
    "\n",
    "#---------------------#start of your code#---------------------#\n",
    "\n",
    "  def getValue(self, state):\n",
    "    \"\"\"\n",
    "      Returns max_action Q(state,action)\n",
    "      where the max is over legal actions.\n",
    "    \"\"\"\n",
    "\n",
    "    possibleActions =\n",
    "    #If there are no legal actions, return 0.0\n",
    "\n",
    "    return #\n",
    "\n",
    "  def getPolicy(self, state):\n",
    "    \"\"\"\n",
    "      Compute the best action to take in a state.\n",
    "\n",
    "    \"\"\"\n",
    "    possibleActions = self.getLegalActions(state)\n",
    "\n",
    "    #If there are no legal actions, return None\n",
    "\n",
    "    return #\n",
    "\n",
    "  def getAction(self, state):\n",
    "    \"\"\"\n",
    "      Compute the action to take in the current state, including exploration.\n",
    "\n",
    "      With probability self.epsilon, we should take a random action.\n",
    "      otherwise - the best policy action (self.getPolicy).\n",
    "\n",
    "      HINT: You might want to use util.flipCoin(prob)\n",
    "      HINT: To pick randomly from a list, use random.choice(list)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Pick Action\n",
    "\n",
    "    #If there are no legal actions, return None\n",
    "\n",
    "    #choose action with epsilon exploration strategy:\n",
    "\n",
    "    return action\n",
    "\n",
    "  def update(self, state, action, nextState, reward):\n",
    "    \"\"\"\n",
    "      You should do your Q-Value update here\n",
    "\n",
    "      NOTE: You should never call this function,\n",
    "      it will be called on your behalf\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #agent parameters\n",
    "    gamma = self.discount\n",
    "    learning_rate = self.alpha\n",
    "\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    self.setQValue(#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IyXE8SzddQ2"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TFzQrJJddQ2"
   },
   "outputs": [],
   "source": [
    "def play_and_train(env,agent,t_max=10**4):\n",
    "    \"\"\"This function should\n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.getAction(s)\n",
    "\n",
    "        next_s,r,done,_ = env.step(a)\n",
    "\n",
    "        #<train(update) agent for state s>\n",
    "\n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhsvXf9BddQ2"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=,epsilon=,discount=,\n",
    "                       getLegalActions = lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZwJKCfvddQ3"
   },
   "source": [
    "Достигните положительной награды, постройте график"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo6QbdGxddQ3"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48CrrrxGddQ3"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env,agent))\n",
    "    agent.epsilon # уменьшайте реплей со временем\n",
    "    if i % 100 ==0:\n",
    "        clear_output(True)\n",
    "        print(agent.epsilon)\n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "gxQu70I_ddQ4"
   },
   "source": [
    "## 3. Continuous state space (2 pt)\n",
    "\n",
    "Чтобы использовать табличный q-learning на continuous состояниях, надо как-то их обрабатывать и бинаризовать. Придумайте способ разбивки на дискретные состояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4eD9gTjddQ4"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "n_actions = env.action_space.n\n",
    "print(\"first state:%s\"%(env.reset()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6V68sMQddQ4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUM1t0jvddQ4"
   },
   "source": [
    "### Play a few games\n",
    "\n",
    "Постройте распределения различных частей состояния игры. Сыграйте несколько игр и запишите все состояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnHeBx5XddQ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zZHwCUfddQ5"
   },
   "source": [
    "## Binarize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6f-DyizddQ5"
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "class Binarizer(ObservationWrapper):\n",
    "\n",
    "    def to_bin(self, value, bins):\n",
    "\n",
    "        return\n",
    "\n",
    "    def _observation(self,state):\n",
    "\n",
    "        state = (self.to_bin(state[0], ), self.to_bin(state[1], ), self.to_bin(state[2], ), self.to_bin(state[3], ))\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpaDIw43ddQ5"
   },
   "outputs": [],
   "source": [
    "env = Binarizer(gym.make(\"CartPole-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkZCEBD_ddQ5"
   },
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sFMtxZYddQ5"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=,epsilon=,discount=,\n",
    "                       getLegalActions = lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2uuuA5bddQ5"
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "rewBuf = []\n",
    "ma = -1000000000000\n",
    "for i in range(10000):\n",
    "    for i in range(100):\n",
    "        rewards.append(play_and_train(env,agent))\n",
    "    agent.epsilon *= #\n",
    "    rewBuf.append(np.mean(rewards[-100:]))\n",
    "    clear_output(True)\n",
    "    print(agent.epsilon)\n",
    "    print(rewBuf[-1])\n",
    "    plt.plot(rewBuf)\n",
    "    if(rewBuf[-1] > 195):\n",
    "        print(\"Win!\")\n",
    "        break\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GCEU6g1ddQ5"
   },
   "source": [
    "## 4. Experience replay (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ_3uojDddQ5"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._replaceId = 0\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Make sure, _storage will not exceed _maxsize.\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "        if len(self._storage) == self._maxsize:\n",
    "            #\n",
    "        else:\n",
    "            #\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        #\n",
    "\n",
    "        return states, actions, rewards, next_states, is_done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KkoMo-EddQ6"
   },
   "source": [
    "Some tests to make sure your buffer works right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze99MZrmddQ6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "replay = ReplayBuffer(2)\n",
    "obj1 = tuple(range(5))\n",
    "obj2 = tuple(range(5, 10))\n",
    "replay.add(*obj1)\n",
    "assert replay.sample(1)==obj1, \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage)==2, \"Please make sure __len__ methods works as intended.\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage)==2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n",
    "assert tuple(np.unique(a) for a in replay.sample(100))==obj2\n",
    "replay.add(*obj1)\n",
    "assert max(len(np.unique(a)) for a in replay.sample(100))==2\n",
    "replay.add(*obj1)\n",
    "assert tuple(np.unique(a) for a in replay.sample(100))==obj1\n",
    "print (\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaImoEEgddQ6"
   },
   "source": [
    "Now let's use this buffer to improve training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czwcYYYmddQ6"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = Binarizer(gym.make('CartPole-v0'))\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChtwaobcddQ6"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=,epsilon=,discount=,\n",
    "                       getLegalActions = lambda s: range(n_actions))\n",
    "replay = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdKXIULcddQ6"
   },
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4, batch_size=10):\n",
    "    \"\"\"This function should\n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        aсtion = agent.getAction(s)\n",
    "        next_s, r, done,_ = env.step(aсtion)\n",
    "\n",
    "        #заполните реплей\n",
    "        #опционально - моежте также как в варианте без реплея обучаться по состояниям которые\n",
    "\n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:break\n",
    "\n",
    "    #learn from replay\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "rcB2f2GaddQ6"
   },
   "source": [
    "Train with experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Lqmk0S_ddQ6"
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "rewBuf = []\n",
    "ma = -1000000000000\n",
    "for i in range(10000):\n",
    "    for i in range(100):\n",
    "        rewards.append(play_and_train(env,agent, batch_size=1000))\n",
    "    agent.epsilon *= #\n",
    "    rewBuf.append(np.mean(rewards[-100:]))\n",
    "    clear_output(True)\n",
    "    print(agent.epsilon)\n",
    "    print(rewBuf[-1])\n",
    "    plt.plot(rewBuf)\n",
    "    if(rewBuf[-1] > 195):\n",
    "        print(\"Win!\")\n",
    "        break\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
